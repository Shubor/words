# Comp3608
[Course website](http://rp-www.cs.usyd.edu.au/~comp3308/)

# Lec01: Intro to AI

## Overview

- Problem solving and search
	- e.g. Route finding problem
	- How can this problem be represented in terms of states, operators and cost function?
	- What search algorithms can be used to find a solution?
	- Are they guaranteed to find a solution if it exists?
	- Is it the optimal one?
	- How long does it take to find a solution?
	- What are the memory requirements?
- Game playing
	- Computers play games by searching a tree, i.e. a search problem (adversarial search)
	- Best move is not possible to calculate (complex, no time to calculate exact consequence), instead make best guess based on past experience and resources
- Machine learning
	- Using a classifier (e.g. set of rules) learned from past data to predict something
- Neural networks
- Clustering
	 - Grouping based on similarity
	- Clustering using SOM (neural network-based approach) not only groups data but also provides useful visualisation
- Probabilistic reasoning and inference
	- Build graphical probabilistic models based on historical data and use them to make predictions for new data

## What is AI

### 4 View Systems

**Think like humans: cognitive modeling approach**

- Require theory on how humans think
- Given a problem, the computer must both solve it correctly as well as taking the same reasoning steps
- e.g. General Problem Solver. Proves theorems, solves geometric problems, plays chess. Order in which it considers goals is similar to humans.
- Cognitive science constructs theories on how human mind works using experimental methods from psychology and computer models (different to AI)

**Think rationally: "Laws of Thought" approach**

- Use logic to build intelligent system
- Take description in logical notation, find solution using _correct inference_ (if it exists)
- Informal knowledge -> formal notation is hard, esp when facts are uncertain

 **Act human: Turing Test approach**

- Human | Computer, Human. Computer passes test if the human can't tell if the responses came from human or system 30% of the time
- Requires: natural language processing, knowledge representation (store what it knows and hears), automated reasoning (use stored info to answer questions and draw conclusions), machine learning (adapt to new situations, detect patterns and apply to new situations)
- e.g. ALICE
- Total Turing: test subject's perceptual abilities by showing video and can also pass physical objects
- Requires: Computer vision, robotics
- Little effort to pass Turing test. Better to concentrate on a systems' performance on practical tasks, rather than the ability to imitate humans
- Reverse Turing: CAPCHA

 **Act rationally: rational agent approach**

- Design of _intelligent agent_ programs (perceives its environment through sensors and acts upon that environment through actuators)
- In built knowledge and goals and perceives environment. Uses this to act rationally (to maximise a performance measure)
- May or may not involve correct inference
	+ e.g. reflex actions such as blinking or recoiling from a hot stove, don't involve reasoning but are rational actions (usually more successful)
- In complex environments during right thing is not always possible (computational demands are too high), and there is limited rationality so the program must act appropriately when there isn't enough time for making computations for a perfect solution

### Strong and Weak AI

- **Weak**: machine can be made to act as if they were intelligent (simulating and acting like humans)
- **Strong**: machines that act intelligently also have real conscious minds (act and think like humans?), this is difficult to define and measure

# Lec02: Problem Solving and Search

## Problem Solving and Search

- Problem solving tasks can be formulated as a search
- Move from initial problem state to goal state following a path
- Involves operators of differing cost and quality
- _Opt solution_ has lowest path cost
- Types of problem formation: incremental, complete state
- Solve problems by searching _state space_ (all states reachable from initial by operators)
	- Generate _search tree_ starting from initial state and applying operators
	- Generate _search graph_ - the same state can be reached from multiple paths

**Problem formation**

1. Initial state
2. Goal state: stated explicitly or implicitly (abstract property or goal test instead of a set of enumerated states, e.g. goal = checkmate)
3. Operators/actions
4. Path cost function: assigns numeric value to each path to reflect performance measure

**Problems with problem formulating - abstraction**

- The formulation is a model, as real problems are too complex and need to be abstracted - simplification via. removal of unnecessary details
- Abstract state = set of real states
- Abstract action = complex combination of real actions
- Abstract solution = set of real paths that are solutions in the real world

- Node != state. Node represents a state and includes parent, children and other relevant information

## Search Strategies

- Tree-search algorithm
	- Exploration of state space by generating successors of the explored states (i.e. expanding)
	- If first node in fringe goal node, stop and return, otherwise, expand
- Search strategy determines _order of node expansion_
- Nodes in fringe ordered based on priority (first = best), so first expanded first
- **Evaluation criteria**
	- Completeness: guaranteed to find sol if it exists?
	- Optimality: guaranteed to find opt sol?
	- Time complexity: how long does it take (in terms of generated nodes)?
	- Space complexity: maximum number of nodes in memory?
- Time and space complexity measured by: _b_ max branching factor, _d_ depth of opt sol, _m_ max depth of state space
- NB: space is a bigger problem than time

**Uninformed (blind) vs informed (heuristic)**

- Uninformed
	- Generates children in systematic way
	- While it knows if a child node is goal or non-goal, it _doesn't know if one non-goal child is better than another_
	- Inefficient in finding solution (time and space)
- Informed uses problem specific knowledge to select order of node expansion
	- _Knows if one non-goal node is better than another_
	- Knowledge incorporated as _evaluation function_ for each node, then most desirable unexpanded node is expanded (_best-first search_)
	- NB: don’t know which is really the best node (we use an evaluation function for this), best-first search expands the node that appears to be best according to the evaluation function
	- Insert children in decreasing order of desirability

## Uninformed Search Strategies

### BFS

- Expands _shallowest_ unexpanded node
- Implementation: put children of the expanded node at the end of the fringe
- Finding solution path requires tracing back from goal
- Properties
	- Complete? yes, if _b_ is finite
	- Optimal? only if step costs are all the same
	- Time? _1 + b + b^2 + b^3 + ... + b^d_ = O(b^d) exponential
		- Search problems with exponential complexity can NOT be solved by uninformed search except for small depth & small branching factor.
	- Space? O(b^d) every node kept in memory

### Uniform Cost (UCS)

- Expands _least-cost unexpanded node g(n)_ from initial state
- Implementation: insert nodes in the fringe in order of increasing path cost _from the root_
- Properties
	- Complete? yes, if cost > 0
	- Optimal? yes
	- Time/space? # nodes with g <= cost of opt O(b^d); depends on path costs not depths. Difficult to characterise in terms of b and d

**UCS and BFS**

- UCS = BFS, when
	- _g(n)_ is a constant, or
	- _g(n) = depth(n)_, or
	- _g(n)_ is the same at each level and increasing as the level increases
	- and when among nodes with the same priority, the left most is expanded first

### DFS

- Expands _deepest_ unexpanded node
- Implementation: insert children at the front of the fringe
- Properties
	- Complete? yes, in finite spaces
	- Optimal? no, can find sol longer than opt even if cost = 1
	- Time? O(b^m)
	- Space? O(bm); once a node has been expanded, it can be removed from memory as soon as all its descendants have been fully explored

### Depth limited (DLS)

- DFS with depth limit _l_, selected based on domain knowledge
- Properties (similar to DFS)
	- Time? O(b^l)
	- Space? O(bl)

### Iterative deepening (IDS)

- Tries all possible depth limits in turn (0, 1, 2, etc.) and applying DFS; Sidesteps the issue of choosing the best depth limit
- Overhead of multiple expansion
	- May seem wasteful as many nodes are expanded multiple times, but for most problems this overhead is small
	- Num BFS expansions: _1 + b + b^2 + ... + b^d_
	- Num IDS expansions: _(d+1) + (d)b + (d-1)b^2 + ... + 1b^d_ (only slightly more)
- IDS preferred when there is a large search space and depth of solution is unknown
- Properties (combines benefits of DFS and BFS)
	- Complete? yes
	- Optimal? yes, if step cost = 1
	- Time? O(b^d); less than the O(b^m) for DFS, _d_ depth of least cost sol, _m_ max depth
	- Space? O(bd)

![Comparison of uninformed search strategies](comp3608/uninformed-search-strategies.png "Comparison of uninformed search strategies")

## Informed Search Strategies

### Greedy

- Evaluation function h(n) for each node = estimated cost from current node _n_ to a goal
- Greedy expands node with smallest _h_ (i.e. node that appears to be closest to a goal)
- Minimises _h_, the estimated cost to a goal
- Properties
	- Complete? yes, in finite space
	- Optimal? no
	- Time? O(b^m); but good heuristic gives drastic improvement
	- Space? O(b^m); every node in memory
- Suppose that we run a greedy search algorithm with h(n) = g(n). (g(n) = cost so far, h(n) estimated cost to the goal), then greedy = BFS search
- NB: h(n) = -g(n) gives DFS, as best nodes will be those with longest path

### A*

- A∗ search expands nodes with minimal _f(n) = g(n) + h(n)_
- Combines UCS and GS
- Properties
    + Complete? Yes, unless there are infinitely many nodes with _f <= f(G)_ where G is optimal
    + Optimal? Yes, with admissible heuristic (if h(n) admissible (tree search) or consistent (graph search))
    + Time? Exponential O(b^d)
    + Space? Exponential, keeps all nodes in memory
- A* with consistent heuristic is optimally efficient (will expand less/equal number of nodes)
- A* with admissible heuristic is optimal
- However completeness != practical completeness, if it takes too long
- Although dominant heuristics are better, it may take a lot of time; might be better to use a simpler heuristic which will expand more nodes but search faster

## Admissible Heuristic

- A heuristic _h(n)_ is admissible, if for every node _n_, _h(n) <= h*(n)_
    + _h*(n)_: true cost to reach goal from n
- Admissible heuristics are optimistic - never overestimates the true distance to a goal
- If _h_ is admissible, then A* is complete and optimal
- Check if _h_ is admissible
    + Don't need to check goal nodes or nodes not on a goal path
- By formulating a _relaxed_ version of the problem and finding the _exact_ solution; this solution is an admissible heuristic
    + Relaxed: problem with fewer restrictions on actions
    + True as optimal solution to original problem, is also a solution to the relaxed version (must be at least as example as the optimal solution in the relaxed version)
- **No single best heuristic**
    + Given _h1, h2, ..., hm_ where none of them dominates
    + Define composite heuristic _h(n) = max{h1(n), h2(n), ..., hm(n)}_
    + At a given node, it uses whichever heuristic is most accurate
- **Consistent (monotonic) heuristic**
    + Consider _ni, nj_ where _ni_ is the parent of _nj_
    + If for all such pairs in the search graph, the following **triangle inequality** is satisfied: _h(ni) <= cost(ni, nj) + h(nj)_ for all n
    + If _h(n)_ consistent, then _f(nj) >= f(ni)_ i.e. _f_ is non decreasing along any path (reverse is also true)
- **Admissibility and consistency**
    + Consistency is stronger
    + If heuristic is consistent, it is also admissible
    + If heuristic is admissible, it is not always consistent
- **Dominance**
    + Given 2 admissible heuristics _h1, h2_, _h2_ dominates _h1_ if for all nodes _h2(n) >= h1(n)_
    + A* using dominant _h2_ will expand fewer nodes than A* using _h1_
    + Dominant heuristics give better estimate of the true cost to G

# Lec03: Local Search Algorithms

## Optimisation Problems

- Each state has a value _v_
- Goal: find optimal state
    + State with highest/lowest _v_ score, depending on what is desirable
- Solution: state, path unimportant
- Large number of states can't be enumerated, can't apply previous algorithms as it is too expensive
- **v-value landscape**
    + Each state has _v_ that can be computed, defined by **heuristic evaluation function** (or objective function)
    + Complete local search: find goal state if one exists
    + Optimal local search: finds state associated with the global max/min

## Local Search Algorithms

### Hill-climbing

- _Iterative improvement_ algorithm
- Idea: keep only a single state in memory, try to improve it
    + Move around trying to find highest peak/lowest dip
    + Don't look ahead beyond immediate neighbors of the current state
- Steepest ascent/descent: goal is max/min value
- Always expands successor with lowest value, no backtracking
- Not complete or optimal, but keeps just one node in memory
- Pro: good for hard, practical problems
    + Little memory
    + Finds reasonable solutions in large spaces
- Con
    + Not clever, can get stuck in a local optimum
    + However, this isn't bad; some local optimums can be reasonably good even though it's not optimal
- Escaping bad local optima
    + Solution found depends on initial
    + If solution isn't good enough - random restart
    + Plateaus: keep track of the number of times _v_ is the same, don't allow revisiting of nodes with same _v_, to avoid visiting same state more than once if algorithm keeps searching even if the values are the same

**Hill-climbing descent example**

1. Select initial (random or given) state _s_, set current node _n_ to _s_
2. Generate successors of _n_, select *n_best* with best v score _v(best)_
3. If _v(best) > v(n)_ return n; compare with parent, if not better stop as local/global min found. Else set _n = n(best)_; accept child and keep searching

### Beam Search

- Keeps track of _k_ best states rather than 1
- Version 1
    + Start with 1 given state
    + At each level, generate all successors of given state
- Version 2
    + State with _k_ randomly generated states
    + At each level, generate all successors of all _k_ states
- For both versions: if any one is goal, stop. Else select the _k_ best successors from list and go to the next level
- Beam search with A*
    + Idea: keep only best _k_ nodes in fringe, i.e. use PQ of size _k_
    + Memory efficient, but neither complete nor optimal

### Simulated Annealing

- Combined hill-stepping (accept best child) with random walk step (accept bad children with some _p_), this can help escape bad local minima
    1. Select initial state _s_, set current node _n_ to _s_. Randomly select _m_ one of _n_'s successors
    2. If _v(m)_ better than _v(n)_, _n = m_; accept child _m_. Else _n = m_ with probability _p_; accept child _m_ with _p_
    3. Repeat until predefined number of iterations is reached, or solution/state reached is good enough
- Probability _p_
    + _p_ decreases exponentially with badness of the move
    + Different ways to define _p_, e.g. _p = e^(v(n)-v(m) / T)_
    + Bad moves are more likely to be allowed at beginning, than at the end
    + Nominator: shows how good child _m_ is, bad move if _v(n) < v(m)_
    + Denominator: parameter _T_ that decreases/anneals over time based on a schedule (high = more bad moves allowed)
- If schedule lowers _T_ slowly enough, algorithm will find global optimum - annealing schedule is very important, but difficult to set

### Genetic Algorithms

- Similar to beam
- Each state = individual, coded as string
- Each state _n_ has fitness score _f(n)_ (evaluation function); higher = better
- Goal: starting with _k_ randomly generated individuals, find optimal state
- Successors pr oduced by selection, crossover, mutation
- At any time, keeps a fixed number of states (population)
- Success depends on representation/encoding
- Easy to implement, but not complete or optimal